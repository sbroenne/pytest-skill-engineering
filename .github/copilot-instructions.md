# Copilot Instructions for pytest-skill-engineering

## CRITICAL: No Backward Compatibility

**NO LEGACY CODE. NO FALLBACKS. CLEAN CODE ONLY.**

- Never add backward compatibility code
- Never add fallback logic for "old" formats
- Never synthesize data that should be explicit
- If something is missing, it's an error - not a fallback opportunity
- Remove legacy code, don't maintain it

## CRITICAL: Never Edit JSON Test Data

**JSON files are TEST OUTPUT. Never hand-edit them.**

- Fixture JSONs (`tests/fixtures/reports/*.json`) are generated by running `tests/fixtures/scenario_*.py`
- Results JSONs (`aitest-reports/*.json`) are generated by running integration/showcase tests
- Hero report JSON is generated by running `tests/showcase/test_hero.py`
- If a JSON file has the wrong format, **re-run the test** that generates it
- If a field is missing or empty, the code that generates it needs fixing ‚Äî not the JSON
- Manually patching JSON masks bugs instead of fixing them

## CRITICAL: Terminology

- **System Prompt** = Instructions given to the agent (what configures agent behavior)
- **Prompt** = What you tell the agent to execute (the test query / user message)

Always say "system prompt" when referring to agent instructions. Never abbreviate to just "prompt".

## CRITICAL: Use uv, Not pip

**This project uses `uv` exclusively. Never use pip.**

- Install packages: `uv add <package>` (not `pip install`)
- Run commands: `uv run <command>` (not direct invocation)
- Install editable: `uv pip install -e .` (not `pip install -e`)
- Sync deps: `uv sync` (not `pip install -r requirements.txt`)

In documentation, always show `uv add` instead of `pip install`.

## CRITICAL: Never Force Commit

**Never use `git commit --no-verify`. Pre-commit hooks exist for a reason.**

- If a hook fails, **fix the issue** and commit normally
- `ruff format` reformats files ‚Üí re-stage and retry
- `pyright` reports errors ‚Üí fix the code
- If a hook is genuinely broken (e.g., false positive with 0 errors), fix the hook config ‚Äî not bypass it
- Force-committing creates a habit of ignoring quality gates

## CRITICAL: What We Test

**We do NOT test agents. We USE agents to test:**
- **MCP Servers** ‚Äî Can an LLM understand and use these tools?
- **CLI Tools** ‚Äî Can an LLM operate this command-line interface?
- **Agent Skills** ‚Äî Does this domain knowledge improve performance?
- **Custom Agents** ‚Äî Do these `.agent.md` instructions produce the right behavior and subagent dispatch?

**System prompts are NOT a standalone test concept.** A custom agent's body IS its system prompt. Testing agent instructions = testing a custom agent file. The `system_prompt=` param on `Agent` exists for raw synthetic tests only ‚Äî not a primary concept.

**The Agent is the test harness**, not the thing being tested. It bundles an LLM provider with the tools/prompts/skills/custom agents you want to evaluate.

## Why This Project Exists

Your MCP server passes all unit tests. Then an LLM tries to use it and:

- Picks the wrong tool
- Passes garbage parameters
- Can't recover from errors
- Ignores your system prompt instructions

**Why?** Because you tested the code, not the AI interface.

For LLMs, your API isn't functions and types ‚Äî it's **tool descriptions, system prompts, skills, and schemas**. These are what the LLM actually sees. Traditional tests can't validate them.

**The key insight: your test is a prompt.** You write what a user would say ("What's my checking balance?"), and the LLM figures out how to use your tools. If it can't, your AI interface needs work.

## CRITICAL: HTML Report Development Workflow

**MANDATORY STEPS AFTER EVERY CODE CHANGE:**

1. **REGENERATE ALL REPORTS** (non-negotiable)
   ```bash
   uv run python scripts/generate_fixture_html.py
   ```
   - Generates fixture reports in docs/reports/
   - Do NOT skip this step

2. **RUN HTML INTEGRATION TESTS** (non-negotiable)
   ```bash
   uv run pytest tests/integration/test_basic_usage.py -q
   ```
   - Verifies end-to-end report generation
   - Fix ALL failures ‚Äî no exceptions

3. **VERIFY CHANGES IN ACTUAL HTML** (non-negotiable)
   ```bash
   Select-String -Path "docs\reports\01_single_agent.html" -Pattern "YOUR_SEARCH_TERM"
   ```
   - Confirm change is present in generated HTML
   - Open in browser when feasible

**WHAT NOT TO DO:**
- ‚ùå Skip report regeneration
- ‚ùå Modify fixture JSONs directly - regenerate via pytest
- ‚ùå Commit without verifying in browser

## Technology Stack & Code Style

### Language & Runtime
- **Python 3.11+** - Use modern syntax (match statements, `|` union types, Self)
- **Fully async** - All agent execution is async (`async def`, `await`)
- **Type hints everywhere** - All public APIs have type annotations
- **`from __future__ import annotations`** - Always at top of every module

### Core Dependencies
| Package | Purpose | Pattern |
|---------|---------|---------|
| `pydantic-ai` | LLM abstraction | Agent execution, MCP toolsets, Azure auth |
| `pydantic-evals` | Evaluation | LLM judge for clarification detection |
| `mcp` | MCP protocol | Server process management, tool discovery |
| `pydantic` | Validation | Config validation (used sparingly) |
| `pytest` | Test framework | Plugin system, fixtures, markers |
| `htpy` | Components | HTML report generation (Python-native) |

### Data Modeling
- **Use `@dataclass(slots=True)`** for all data objects
- **Use `frozen=True`** for immutable config (Wait, Provider)
- **Use `TypedDict`** for template data contracts (see `components/types.py`)
- **No Pydantic models** for core types - just dataclasses

```python
# Good - immutable config with slots
@dataclass(slots=True, frozen=True)
class Wait:
    strategy: WaitStrategy
    timeout_ms: int = 30000

# Good - mutable data with slots
@dataclass(slots=True)
class AgentResult:
    turns: list[Turn]
    success: bool
```

### Async Patterns
- **pytest-asyncio auto mode** - Tests are async by default (no `@pytest.mark.asyncio`)
- **Use `asyncio.TaskGroup`** for parallel operations (Python 3.11+)
- **Context managers** - Use `async with` for server lifecycle

```python
# Tests are async by default (asyncio_mode = "auto" in pyproject.toml)
async def test_balance(aitest_run, banking_server):
    result = await aitest_run(agent, "What's my balance?")
    assert result.success
```

### Build & Quality Tools
| Tool | Config | Purpose |
|------|--------|---------|
| `uv` | `pyproject.toml` | Package management, virtual envs |
| `hatch` | `pyproject.toml` | Build backend |
| `ruff` | `pyproject.toml` | Linting + formatting (replaces black, isort, flake8) |
| `pyright` | `pyproject.toml` | Type checking (basic mode) |
| `pre-commit` | `.pre-commit-config.yaml` | Git hooks |

### Commands
```bash
# Run lints
uv run ruff check src tests

# Fix lint issues
uv run ruff check --fix src tests

# Format code
uv run ruff format src tests

# Type check
uv run pyright src

# Run unit tests (fast, no LLM)
uv run pytest tests/unit -v

# Run integration tests (slow, uses LLM)
uv run pytest tests/integration -v
```

### Import Conventions
- Group imports: stdlib ‚Üí third-party ‚Üí local
- Use `TYPE_CHECKING` block for type-only imports
- Import from package root when possible (`from pytest_skill_engineering import Agent`)

```python
from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

import pydantic_ai
from mcp import ClientSession

from pytest_skill_engineering.core.result import AgentResult

if TYPE_CHECKING:
    from pytest_skill_engineering.core.agent import Agent
```

## What We're Building

**pytest-skill-engineering** is a pytest plugin for testing MCP servers and CLIs. You write tests as natural language prompts, and an LLM executes them against your tools. Reports tell you **what to fix**, not just **what failed**.

### Core Features

1. **Base Testing**: Define test agents, run tests against MCP/CLI tool servers
   - Agent = Provider (LLM) + System Prompt + MCP/CLI Servers + optional Skill + optional Custom Agents
   - Use `aitest_run` fixture to execute agent and verify tool usage
   - Assert on `result.success`, `result.tool_was_called("tool_name")`, `result.final_response`

2. **Agent Leaderboard**: When you test multiple agents, the report shows a leaderboard
   - 1 agent ‚Üí Just results
   - Multiple agents ‚Üí Agent Leaderboard (always)
   - AI detects what varies (Model, Skill, Custom Agent, Server) to focus its analysis

3. **Winning Criteria**: Highest pass rate ‚Üí Lowest cost (tiebreaker)
   - Use `--aitest-min-pass-rate=N` to fail the session if overall pass rate falls below N%

4. **Multi-Turn Sessions**: Test conversations that build on context
   - Use `@pytest.mark.session("session-name")` on test class
   - Tests share agent state within the session
   - Reports track session flow and context continuity

5. **Skill Testing**: Validate agent domain knowledge
   - Load skills from markdown files with `Skill.from_path()`
   - Skills inject structured knowledge into agent context
   - Reports analyze skill effectiveness and suggest improvements

6. **Custom Agent Testing**: Test `.agent.md` custom agent files (VS Code / Claude Code format)
   - `Agent.from_agent_file(path, provider, ...)` ‚Äî agent file becomes the system prompt; test it synthetically
   - `load_custom_agent(path)` + `CopilotAgent(custom_agents=[...])` ‚Äî test real subagent dispatch through Copilot
   - Both approaches are first-class: same prominence as Skill Testing

7. **Clarification Detection**: Catch agents that ask questions instead of acting
   - LLM-as-judge detects "Would you like me to...?" style responses
   - Configure with `ClarificationDetection(enabled=True)` on Agent
   - Assert with `result.asked_for_clarification` / `result.clarification_count`
   - Levels: INFO (log only), WARNING (default), ERROR (fail test)
   - Uses separate judge LLM call (defaults to agent's own model)

### AI Analysis (KEY DIFFERENTIATOR)

Reports are **insights-first**, not metrics-first. AI analysis is **mandatory** when generating reports.

Reports include:
- **üéØ Recommendation**: Deploy recommendation with cost/performance analysis
- **‚ùå Failure Analysis**: Root cause + suggested fix for each failure
- **üîß MCP Tool Feedback**: Improve tool descriptions, with copy button
- **üìù Prompt Feedback**: System prompt improvements
- **üìö Skill Feedback**: Skill restructuring suggestions
- **‚ö° Optimizations**: Reduce turns/tokens

```bash
# Run tests with AI analysis (mandatory --aitest-summary-model)
pytest tests/ --aitest-html=report.html --aitest-summary-model=azure/gpt-5.2-chat

# Regenerate report with new AI insights from existing JSON (no re-run)
pytest-skill-engineering-report results.json --html report.html --summary --summary-model azure/gpt-5-mini
```

### Key Types

```python
from pytest_skill_engineering import Agent, Provider, MCPServer, Skill, load_system_prompts, load_custom_agent

# Define an agent (auth via AZURE_API_BASE env var)
agent = Agent(
    provider=Provider(model="azure/gpt-5-mini"),
    mcp_servers=[my_server],
    system_prompt="You are helpful...",
    skill=Skill.from_path("skills/financial-advisor"),  # Optional domain knowledge
    max_turns=10,
)

# Load a custom agent file as the agent under test (synthetic testing)
agent = Agent.from_agent_file(
    "skills/my-skill/agent.md",
    provider=Provider(model="azure/gpt-5-mini"),
    mcp_servers=[my_server],
)

# Load a custom agent as a subagent for CopilotAgent (real Copilot dispatch)
from pytest_skill_engineering.copilot import CopilotAgent
copilot_agent = CopilotAgent(
    custom_agents=[load_custom_agent("skills/my-skill/agent.md")],
)

# Load system prompts from .md files - returns dict[str, str]
prompts = load_system_prompts(Path("prompts/"))
# {"concise": "Be brief...", "detailed": "Explain..."}

# Run test
result = await aitest_run(agent, "Do something with tools")
assert result.success
assert result.tool_was_called("my_tool")
assert not result.asked_for_clarification  # Agent should act, not ask
```

### Multi-Turn Sessions

```python
@pytest.mark.session("banking-flow")
class TestBankingWorkflow:
    async def test_check_balance(self, aitest_run, bank_agent):
        result = await aitest_run(bank_agent, "What's my balance?")
        assert result.success

    async def test_transfer(self, aitest_run, bank_agent):
        # Shares context with previous test
        result = await aitest_run(bank_agent, "Transfer $100 to savings")
        assert result.tool_was_called("transfer")
```

### System Prompts

System prompts can be plain `.md` files or YAML files with metadata.

```python
# Load .md files as plain strings - returns dict[str, str]
prompts = load_system_prompts(Path("prompts/"))
# {"concise": "Be brief...", "detailed": "Explain..."}

# Or load .yaml files as Prompt objects - returns list[Prompt]
from pytest_skill_engineering import load_prompts
prompt_list = load_prompts(Path("prompts/"))

# Use with pytest parametrize
@pytest.mark.parametrize("prompt_name,system_prompt", prompts.items())
async def test_with_prompt(aitest_run, banking_server, prompt_name, system_prompt):
    agent = Agent(
        provider=Provider(model="azure/gpt-5-mini"),
        mcp_servers=[banking_server],
        system_prompt=system_prompt,
    )
    result = await aitest_run(agent, "What's my balance?")
    assert result.success
```

## CRITICAL: Testing Philosophy

**Unit tests with mocks are WORTHLESS for this project.**

This is a testing framework that uses LLMs to test tools, prompts, and skills. The only way to verify it works is to:
1. Run **real integration tests** against **real LLM providers**
2. Use **actual MCP/CLI servers** that perform real operations
3. Verify the **full pipeline end-to-end**

### What NOT to do:
- Do NOT write unit tests with mocked LLM responses
- Do NOT claim "tests pass" when tests only mock the core functionality
- Do NOT use `unittest.mock.patch` on PydanticAI or agent execution
- Do NOT declare a feature "done" or "working" based only on unit tests passing
- Fast test execution (< 1 second) is a RED FLAG - real LLM calls take time

### What TO do:
- Write integration tests that call real Azure OpenAI / OpenAI / Copilot models
- Use the cheapest available model (check Azure subscription first)
- Test with the Banking or Todo MCP server (built-in test harnesses)
- Verify actual tool calls happen and produce expected results
- Accept that integration tests take 5-30+ seconds per test
- Run integration tests BEFORE declaring a feature complete

## CRITICAL: No Such Thing as a Pre-Existing Failure

**Every test failure is YOUR responsibility to fix. There are no "pre-existing" or "unrelated" failures.**

- NEVER skip a failing test because it was failing before your change
- NEVER label failures as "pre-existing" and move on
- NEVER say "this failure is unrelated to my change"
- If a test fails, fix it ‚Äî full stop
- If a test is wrong (not the code), fix the test
- The baseline must be **zero failures** before and after your change

## CRITICAL: Efficient Test Execution

**Integration tests are EXPENSIVE. Never re-run passing tests unnecessarily.**

### pytest caching commands:
```bash
# Run ONLY tests that failed last time (MOST COMMON)
pytest --lf tests/integration/

# Run failed tests first, then the rest
pytest --ff tests/integration/

# Check what's in the cache (see last failures)
pytest --cache-show

# Clear the cache (fresh start)
pytest --cache-clear

# Run specific failing test(s) only
pytest tests/integration/test_foo.py::TestClass::test_name -v
```

### Rules for the AI assistant:
1. **After making changes, run the relevant integration tests** ‚Äî not unit tests
2. **After fixing a test, run ONLY that specific test** to confirm
3. **Use `--lf` to re-run only failed tests** after a full run
4. **Fix ALL failures** ‚Äî no exceptions, no "pre-existing" excuses
5. **Quote the specific test paths when running individual tests**

## Azure Configuration

**Endpoint**: `https://stbrnner1.cognitiveservices.azure.com/`
**Resource Group**: `rg_foundry`
**Account**: `stbrnner1`

**Authentication**: Entra ID (automatic via `az login`). No API keys needed!
The engine uses `core.auth.get_azure_ad_token_provider()` internally (shared module).

Available models (checked 2026-02-01):
- `gpt-5-mini` - CHEAPEST, use for most tests
- `gpt-5.2-chat` - More capable
- `gpt-4.1` - Most capable

Check for updates:
```bash
az cognitiveservices account deployment list \
  --name stbrnner1 \
  --resource-group rg_foundry \
  -o table
```

## Copilot Model Provider

Use the `copilot/` prefix to route LLM calls through the Copilot SDK instead of Azure/OpenAI.
Requires `pytest-skill-engineering[copilot]` and Copilot auth (`gh auth login` or `GITHUB_TOKEN`).

```bash
# Use Copilot for AI insights
pytest tests/ --aitest-summary-model=copilot/gpt-5-mini

# Use Copilot for llm_assert / llm_score
pytest tests/ --llm-model=copilot/gpt-5-mini
```

Handled in `build_model_from_string()` in `pydantic_adapter.py` ‚Äî creates a `CopilotModel` from `copilot/model.py`.
Available models are dynamic (whatever Copilot exposes).

## Project Structure

```
src/pytest_skill_engineering/
‚îú‚îÄ‚îÄ core/                  # Core types
‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # Agent, Provider, MCPServer, CLIServer, Wait
‚îÇ   ‚îú‚îÄ‚îÄ auth.py            # Shared Azure AD auth (get_azure_ad_token_provider)
‚îÇ   ‚îú‚îÄ‚îÄ prompt.py          # load_system_prompts() for .md files (returns dict[str, str])
‚îÇ   ‚îú‚îÄ‚îÄ result.py          # AgentResult, Turn, ToolCall, ToolInfo, SkillInfo
‚îÇ   ‚îú‚îÄ‚îÄ skill.py           # Skill, load from markdown
‚îÇ   ‚îî‚îÄ‚îÄ errors.py          # AITestError, ServerStartError, etc.
‚îú‚îÄ‚îÄ execution/             # Runtime
‚îÇ   ‚îú‚îÄ‚îÄ engine.py          # AgentEngine (PydanticAI-powered agent execution)
‚îÇ   ‚îú‚îÄ‚îÄ pydantic_adapter.py # Adapter: our config types ‚Üî PydanticAI types (handles azure/, copilot/ prefixes)
‚îÇ   ‚îú‚îÄ‚îÄ cli_toolset.py     # Custom PydanticAI Toolset for CLI servers
‚îÇ   ‚îú‚îÄ‚îÄ clarification.py   # Clarification detection via pydantic-evals LLMJudge
‚îÇ   ‚îú‚îÄ‚îÄ servers.py         # Server process management
‚îÇ   ‚îî‚îÄ‚îÄ skill_tools.py     # Skill injection into agent
‚îú‚îÄ‚îÄ fixtures/              # Pytest fixtures
‚îÇ   ‚îú‚îÄ‚îÄ run.py             # aitest_run fixture
‚îÇ   ‚îî‚îÄ‚îÄ factories.py       # skill_factory (Skills only - agents created inline)
‚îú‚îÄ‚îÄ reporting/             # AI analysis & reports
‚îÇ   ‚îú‚îÄ‚îÄ collector.py       # TestReport, SuiteReport dataclasses + build_suite_report()
‚îÇ   ‚îú‚îÄ‚îÄ generator.py       # generate_html(), generate_json(), generate_mermaid_sequence()
‚îÇ   ‚îú‚îÄ‚îÄ insights.py        # AI analysis engine (PydanticAI-powered) ‚Üí InsightsResult
‚îÇ   ‚îî‚îÄ‚îÄ components/        # htpy report components
‚îÇ       ‚îú‚îÄ‚îÄ types.py       # TypedDicts for component data shapes
‚îÇ       ‚îú‚îÄ‚îÄ report.py      # Full report layout
‚îÇ       ‚îú‚îÄ‚îÄ agent_leaderboard.py  # Agent ranking table
‚îÇ       ‚îú‚îÄ‚îÄ agent_selector.py     # Agent comparison toggles
‚îÇ       ‚îú‚îÄ‚îÄ test_grid.py          # Test results grid
‚îÇ       ‚îú‚îÄ‚îÄ test_comparison.py    # Side-by-side agent comparison
‚îÇ       ‚îî‚îÄ‚îÄ overlay.py            # Fullscreen expanded view
‚îú‚îÄ‚îÄ templates/             # Static assets for HTML reports
‚îÇ   ‚îî‚îÄ‚îÄ partials/          # Static assets
‚îÇ       ‚îú‚îÄ‚îÄ report.css     # Hand-written CSS (edit directly)
‚îÇ       ‚îî‚îÄ‚îÄ scripts.js     # JS (Mermaid, copy buttons, filtering)
‚îî‚îÄ‚îÄ testing/               # Test harnesses
    ‚îú‚îÄ‚îÄ todo.py            # TodoStore for CRUD tests
    ‚îú‚îÄ‚îÄ todo_mcp.py        # Todo MCP server
    ‚îú‚îÄ‚îÄ banking.py         # BankingService for financial tests
    ‚îî‚îÄ‚îÄ banking_mcp.py     # Banking MCP server

tests/
‚îú‚îÄ‚îÄ integration/           # REAL LLM tests (the only tests that matter)
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py        # Constants + server fixtures (agents created inline)
‚îÇ   ‚îú‚îÄ‚îÄ test_basic_usage.py        # Base functionality
‚îÇ   ‚îú‚îÄ‚îÄ test_dimension_detection.py # Proves auto-detection works (all permutations)
‚îÇ   ‚îú‚îÄ‚îÄ test_skills.py             # Skill testing
‚îÇ   ‚îú‚îÄ‚îÄ test_skill_improvement.py  # Skill before/after comparisons
‚îÇ   ‚îú‚îÄ‚îÄ test_sessions.py           # Multi-turn sessions
‚îÇ   ‚îú‚îÄ‚îÄ test_ai_summary.py         # AI insights generation
‚îÇ   ‚îú‚îÄ‚îÄ test_ab_servers.py         # Server A/B testing
‚îÇ   ‚îú‚îÄ‚îÄ test_cli_server.py         # CLI server testing
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                   # Plain .md system prompt files
‚îÇ   ‚îî‚îÄ‚îÄ skills/                    # Test skills
‚îî‚îÄ‚îÄ unit/                  # Pure logic only (no mocking LLMs)
```

## Test Configuration (conftest.py)

Integration tests use centralized constants from `tests/integration/conftest.py`:

```python
# Models
DEFAULT_MODEL = "gpt-5-mini"           # Cheapest, use for most tests
BENCHMARK_MODELS = ["gpt-5-mini", "gpt-4.1-mini"]  # For model comparison

# Rate limits (Azure deployments)
DEFAULT_RPM = 10
DEFAULT_TPM = 10000

# Turn limits
DEFAULT_MAX_TURNS = 5

# Server fixtures: todo_server, banking_server
# Agents are created INLINE in each test using these constants
```

**Pattern for writing tests:**
```python
from pytest_skill_engineering import Agent, Provider
from .conftest import DEFAULT_MODEL, DEFAULT_RPM, DEFAULT_TPM, DEFAULT_MAX_TURNS

async def test_balance(aitest_run, banking_server):
    agent = Agent(
        provider=Provider(model=f"azure/{DEFAULT_MODEL}", rpm=DEFAULT_RPM, tpm=DEFAULT_TPM),
        mcp_servers=[banking_server],
        system_prompt="You are a banking assistant.",
        max_turns=DEFAULT_MAX_TURNS,
    )
    result = await aitest_run(agent, "What's my checking balance?")
    assert result.success
```

## Semantic Assertions with llm_assert

Use the built-in `llm_assert` fixture for AI-powered semantic assertions (powered by `pydantic-evals` LLMJudge):

```python
async def test_response_quality(aitest_run, banking_server, llm_assert):
    agent = Agent(...)
    result = await aitest_run(agent, "Show me my account balances and recent transactions")
    
    # Semantic assertion - AI evaluates if condition is met
    assert llm_assert(result.final_response, "includes account balances and transaction details")
```

## CRITICAL: Report Development

### Report Generation Architecture

The report pipeline flows:  
**Test Execution ‚Üí Plugin (list[TestReport]) ‚Üí build_suite_report() ‚Üí AI Insights ‚Üí generate_html()/generate_json() ‚Üí HTML/JSON**

```
src/pytest_skill_engineering/reporting/
‚îú‚îÄ‚îÄ collector.py       # TestReport, SuiteReport dataclasses + build_suite_report()
‚îú‚îÄ‚îÄ insights.py        # AI analysis engine (mandatory) ‚Üí InsightsResult
‚îú‚îÄ‚îÄ generator.py       # generate_html(), generate_json() module-level functions
‚îî‚îÄ‚îÄ components/        # htpy report components + types.py
```

### Data Contracts (IMPORTANT)

Every htpy component has a **typed data contract** in `components/types.py`. This is the explicit interface between Python and components:

| Contract | Used By | Purpose |
|----------|---------|---------|
| `AgentData` | `agent_leaderboard.py`, `agent_selector.py` | Agent metrics: pass_rate, cost, tokens, is_winner |
| `TestResultData` | `test_comparison.py`, `test_grid.py` | Per-agent test result: tool_calls, mermaid, outcome |
| `TestData` | `test_grid.py` | Test with `results_by_agent` dict |
| `TestGroupData` | `test_grid.py` | Session or standalone group containing tests |
| `ReportContext` | `report.py` | Full report context with all data + helper functions |

**Pattern**: When modifying components, always check `components/types.py` for the expected shape. When adding component fields, add them to the contract first.

### Template Structure

```
src/pytest_skill_engineering/templates/
‚îî‚îÄ‚îÄ partials/
    ‚îú‚îÄ‚îÄ report.css           # Hand-written CSS (edit directly)
    ‚îî‚îÄ‚îÄ scripts.js           # JS: Mermaid, copy buttons, expand/collapse, agent filtering
```

**Note:** HTML rendering is handled by htpy components in `reporting/components/`, not by template files.

### CSS Development

The project uses hand-written CSS in `partials/report.css`. No build step needed ‚Äî edit and regenerate reports.

The CSS provides:
- Design tokens as CSS custom properties (colors, shadows, radii, fonts)
- Utility classes matching the patterns used by htpy components
- Semantic component classes (`.card`, `.leaderboard-table`, `.agent-chip`, etc.)
- Markdown content styling

To modify styles:
1. Edit `src/pytest_skill_engineering/templates/partials/report.css`
2. Regenerate report: `uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html`
3. Open in browser and verify

### Report Sources

1. **Integration tests demonstrate capabilities** - Each test file shows a feature:
   - `test_basic_usage.py` ‚Üí Basic tool usage
   - `test_model_benchmark.py` ‚Üí Model comparison leaderboard
   - `test_prompt_arena.py` ‚Üí System prompt comparison
   - `test_matrix.py` ‚Üí Model √ó System Prompt grid
   - `test_skills.py` ‚Üí Skills integration
   - `test_sessions.py` ‚Üí Multi-turn sessions
   - `test_cli_server.py` ‚Üí CLI server testing

2. **Showcase tests for hero report** - Located in `tests/showcase/`:
   - `test_hero.py` ‚Üí Curated tests for README showcase
   - Demonstrates ALL capabilities in a single cohesive report
   - Output: `docs/demo/hero-report.html` (committed to repo)

3. **Report config is in pyproject.toml**:
   ```toml
   addopts = """
   --aitest-summary-model=azure/gpt-5.2-chat
   --aitest-html=aitest-reports/report.html
   """
   ```

### Generate Reports

```bash
# Integration tests (development)
pytest tests/integration/ -v

# Hero report for README showcase
pytest tests/showcase/ -v --aitest-html=docs/demo/hero-report.html
```

### Manual Testing Workflow (Pre-PR)

**Integration tests use real LLM calls and are expensive. Run the relevant integration tests after every change.**

```bash
# 1. Run the integration test(s) that cover your change
uv run pytest tests/integration/test_basic_usage.py -v

# 2. Run all failed tests (if a broader run previously failed)
uv run pytest --lf tests/integration/ -v

# 3. Generate hero report (before major releases)
uv run pytest tests/showcase/ -v --aitest-html=docs/demo/hero-report.html
```

## CRITICAL: Template/Report Changes - NEVER Re-run Tests

**For template, CSS, JS, or report generation changes:**

```bash
# CORRECT - Regenerate HTML from existing JSON (instant, no LLM cost)
uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html

# WRONG - Never re-run tests just to see template changes!
# uv run pytest tests/showcase/ ...  ‚Üê DON'T DO THIS
```

**When to re-run tests:**
- Test code itself changed
- Agent/tool/engine functionality changed
- Need fresh data with different models/prompts

**When to regenerate from JSON:**
- Template changes (HTML, CSS, JS)
- Report generator code changes
- Layout/styling experiments
- Data contract changes (if backward compatible)

**Workflow for template development:**
1. Edit CSS in `src/pytest_skill_engineering/templates/partials/report.css` or htpy components
2. Regenerate report from existing JSON:
   ```bash
   uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html
   ```
3. Open the HTML file in browser
4. Repeat steps 1-3 until satisfied

This workflow is **instant and free** - no LLM calls, no API costs.

### Key Files for Report Development

| File | When to Modify |
|------|----------------|
| `reporting/components/types.py` | Adding new data fields for components |
| `reporting/generator.py` | Changing how context is built for components |
| `reporting/components/*.py` | Individual UI components (htpy) |
| `templates/partials/scripts.js` | Interactivity, Mermaid diagrams |
| `templates/partials/report.css` | Styles, colors, layout |
| `cli.py` | CLI commands for report regeneration |

### Report Design Principles

- **Material Design** - Match mkdocs-material indigo theme
- **Roboto fonts** - Via Google Fonts
- **Test details expanded by default** - Users want to see results immediately
- **Human-readable names everywhere** - All user-facing reports (HTML, Markdown) must use human-readable names, not internal IDs. Use docstrings or humanize function names for tests. Use `agent_name` (not `agent_id`) for agents. Use `system_prompt_name` (not raw keys). If a human-readable name isn't available, humanize the ID (e.g., `test_check_balance` ‚Üí `Test check balance`).
- **Assertions visible** - Show tool_was_called, semantic assertions
- **Mermaid diagrams readable** - Use neutral theme with good contrast
- **AI insights prominent** - Verdict section at top with clear recommendation
- **Contract-first development** - Define TypedDict before touching templates

### Markdown Style Guidelines

- **No horizontal rules (`---`)** - Headings provide sufficient visual separation
- Use `##` headings for major sections, `###` for subsections
- Horizontal rules inside code blocks are fine (e.g., YAML frontmatter examples)


