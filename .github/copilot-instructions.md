# Copilot Instructions for pytest-skill-engineering

## CRITICAL: No Backward Compatibility

**NO LEGACY CODE. NO FALLBACKS. CLEAN CODE ONLY.**

- Never add backward compatibility code
- Never add fallback logic for "old" formats
- Never synthesize data that should be explicit
- If something is missing, it's an error - not a fallback opportunity
- Remove legacy code, don't maintain it

## CRITICAL: Never Edit JSON Test Data

**JSON files are TEST OUTPUT. Never hand-edit them.**

- Fixture JSONs (`tests/fixtures/reports/*.json`) are generated by running `tests/fixtures/scenario_*.py`
- Results JSONs (`aitest-reports/*.json`) are generated by running integration/showcase tests
- Hero report JSON is generated by running `tests/showcase/test_hero.py`
- If a JSON file has the wrong format, **re-run the test** that generates it
- If a field is missing or empty, the code that generates it needs fixing ‚Äî not the JSON
- Manually patching JSON masks bugs instead of fixing them

## CRITICAL: Terminology

- **System Prompt** = Instructions given to the agent (what configures agent behavior)
- **Prompt** = What you tell the agent to execute (the test query / user message)
- **Custom Agent** = A `.agent.md` file that defines a specialist agent (name, description, instructions, tools). It is a **definition**, not a runtime concept.
- **Subagent Dispatch** = The runtime mechanism where Copilot's orchestrator routes a task to a custom agent. A custom agent becomes a subagent *when dispatched to* ‚Äî but it is NOT inherently a "subagent."
- **Eval** = The test harness/configuration (`Eval` or `CopilotEval`). Not the thing being tested.
- **Coding Agent** = The real GitHub Copilot agent that runs user sessions.

Always say "system prompt" when referring to agent instructions. Never abbreviate to just "prompt".

**Custom agent ‚â† subagent.** Never use these interchangeably:
- `load_custom_agent()` loads a custom agent definition ‚Äî it does NOT create a subagent
- `custom_agents=` on `CopilotEval` registers custom agents ‚Äî Copilot *may* dispatch to them as subagents at runtime
- Write "custom agent dispatch" (not "subagent dispatch") when describing what CopilotEval tests
- Only use "subagent" when specifically describing the runtime event (`result.subagent_invocations`)

## CRITICAL: Use uv, Not pip

**This project uses `uv` exclusively. Never use pip.**

- Install packages: `uv add <package>` (not `pip install`)
- Run commands: `uv run <command>` (not direct invocation)
- Install editable: `uv pip install -e .` (not `pip install -e`)
- Sync deps: `uv sync` (not `pip install -r requirements.txt`)

In documentation, always show `uv add` instead of `pip install`.

## CRITICAL: Never Force Commit

**Never use `git commit --no-verify`. Pre-commit hooks exist for a reason.**

- If a hook fails, **fix the issue** and commit normally
- `ruff format` reformats files ‚Üí re-stage and retry
- `pyright` reports errors ‚Üí fix the code
- If a hook is genuinely broken (e.g., false positive with 0 errors), fix the hook config ‚Äî not bypass it
- Force-committing creates a habit of ignoring quality gates

## CRITICAL: What We Test

**We do NOT test agents. We USE agents to test:**
- **MCP Servers** ‚Äî Can an LLM understand and use these tools?
- **CLI Tools** ‚Äî Can an LLM operate this command-line interface?
- **Eval Skills** ‚Äî Does this domain knowledge improve performance?
- **Custom Agents** ‚Äî Do these `.agent.md` instructions produce the right behavior and subagent dispatch?

**System prompts are NOT a standalone test concept.** A custom agent's body IS its system prompt. Testing agent instructions = testing a custom agent file. The `system_prompt=` param on `Eval` exists for raw synthetic tests only ‚Äî not a primary concept.

**The Eval is the test harness**, not the thing being tested. It bundles an LLM provider with the tools/prompts/skills/custom agents you want to evaluate.

## Why This Project Exists

Your MCP server passes all unit tests. Then an LLM tries to use it and:

- Picks the wrong tool
- Passes garbage parameters
- Can't recover from errors
- Ignores your system prompt instructions

**Why?** Because you tested the code, not the AI interface.

For LLMs, your API isn't functions and types ‚Äî it's **tool descriptions, system prompts, skills, and schemas**. These are what the LLM actually sees. Traditional tests can't validate them.

**The key insight: your test is a prompt.** You write what a user would say ("What's my checking balance?"), and the LLM figures out how to use your tools. If it can't, your AI interface needs work.

## CRITICAL: HTML Report Development Workflow

**MANDATORY STEPS AFTER EVERY CODE CHANGE:**

1. **REGENERATE ALL REPORTS** (non-negotiable)
   ```bash
   uv run python scripts/generate_fixture_html.py
   ```
   - Generates fixture reports in docs/reports/
   - Do NOT skip this step

2. **RUN HTML INTEGRATION TESTS** (non-negotiable)
   ```bash
   uv run python -m pytest tests/integration/pydantic/test_01_basic.py -q
   ```
   - Verifies end-to-end report generation
   - Fix ALL failures ‚Äî no exceptions

3. **VERIFY CHANGES IN ACTUAL HTML** (non-negotiable)
   ```bash
   Select-String -Path "docs\reports\01_single_agent.html" -Pattern "YOUR_SEARCH_TERM"
   ```
   - Confirm change is present in generated HTML
   - Open in browser when feasible

**WHAT NOT TO DO:**
- ‚ùå Skip report regeneration
- ‚ùå Modify fixture JSONs directly - regenerate via pytest
- ‚ùå Commit without verifying in browser

## Technology Stack & Code Style

### Language & Runtime
- **Python 3.11+** - Use modern syntax (match statements, `|` union types, Self)
- **Fully async** - All eval execution is async (`async def`, `await`)
- **Type hints everywhere** - All public APIs have type annotations
- **`from __future__ import annotations`** - Always at top of every module

### Core Dependencies
| Package | Purpose | Pattern |
|---------|---------|---------|
| `pydantic-ai` | LLM abstraction | Eval execution, MCP toolsets, Azure auth |
| `pydantic-evals` | Evaluation | LLM judge for clarification detection |
| `mcp` | MCP protocol | Server process management, tool discovery |
| `pydantic` | Validation | Config validation (used sparingly) |
| `pytest` | Test framework | Plugin system, fixtures, markers |
| `htpy` | Components | HTML report generation (Python-native) |

### Data Modeling
- **Use `@dataclass(slots=True)`** for all data objects
- **Use `frozen=True`** for immutable config (Wait, Provider)
- **Use `TypedDict`** for template data contracts (see `components/types.py`)
- **No Pydantic models** for core types - just dataclasses

```python
# Good - immutable config with slots
@dataclass(slots=True, frozen=True)
class Wait:
    strategy: WaitStrategy
    timeout_ms: int = 30000

# Good - mutable data with slots
@dataclass(slots=True)
class EvalResult:
    turns: list[Turn]
    success: bool
```

### Async Patterns
- **pytest-asyncio auto mode** - Tests are async by default (no `@pytest.mark.asyncio`)
- **Use `asyncio.TaskGroup`** for parallel operations (Python 3.11+)
- **Context managers** - Use `async with` for server lifecycle

```python
# Tests are async by default (asyncio_mode = "auto" in pyproject.toml)
async def test_balance(eval_run, banking_server):
    result = await eval_run(agent, "What's my balance?")
    assert result.success
```

### Build & Quality Tools
| Tool | Config | Purpose |
|------|--------|---------|
| `uv` | `pyproject.toml` | Package management, virtual envs |
| `hatch` | `pyproject.toml` | Build backend |
| `ruff` | `pyproject.toml` | Linting + formatting (replaces black, isort, flake8) |
| `pyright` | `pyproject.toml` | Type checking (basic mode) |
| `pre-commit` | `.pre-commit-config.yaml` | Git hooks |

### Commands
```bash
# Run lints
uv run ruff check src tests

# Fix lint issues
uv run ruff check --fix src tests

# Format code
uv run ruff format src tests

# Type check
uv run pyright src

# Run pydantic integration tests (ALWAYS use these, not unit tests)
uv run python -m pytest tests/integration/pydantic/ -v

# Re-run only failures
uv run python -m pytest --lf tests/integration/pydantic/ -v
```

### Import Conventions
- Group imports: stdlib ‚Üí third-party ‚Üí local
- Use `TYPE_CHECKING` block for type-only imports
- Import from package root when possible (`from pytest_skill_engineering import Eval`)

```python
from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

import pydantic_ai
from mcp import ClientSession

from pytest_skill_engineering.core.result import EvalResult

if TYPE_CHECKING:
    from pytest_skill_engineering.core.eval import Eval
```

## What We're Building

**pytest-skill-engineering** is a pytest plugin for testing MCP servers and CLIs. You write tests as natural language prompts, and an LLM executes them against your tools. Reports tell you **what to fix**, not just **what failed**.

### Core Features

1. **Base Testing**: Define evals, run tests against MCP/CLI tool servers
   - Eval = Provider (LLM) + System Prompt + MCP/CLI Servers + optional Skill + optional Custom Agents
   - Use `eval_run` fixture to execute eval and verify tool usage
   - Assert on `result.success`, `result.tool_was_called("tool_name")`, `result.final_response`

2. **Eval Leaderboard**: When you test multiple evals, the report shows a leaderboard
   - 1 eval ‚Üí Just results
   - Multiple evals ‚Üí Eval Leaderboard (always)
   - AI detects what varies (Model, Skill, Custom Agent, Server) to focus its analysis

3. **Winning Criteria**: Highest pass rate ‚Üí Lowest cost (tiebreaker)
   - Use `--aitest-min-pass-rate=N` to fail the session if overall pass rate falls below N%

4. **Multi-Turn Sessions**: Test conversations that build on context
   - Use `@pytest.mark.session("session-name")` on test class
   - Tests share eval state within the session
   - Reports track session flow and context continuity

5. **Skill Testing**: Validate eval domain knowledge
   - Load skills from markdown files with `Skill.from_path()`
   - Skills inject structured knowledge into eval context
   - Reports analyze skill effectiveness and suggest improvements

6. **Custom Agent Testing**: Test `.agent.md` custom agent files (VS Code / Claude Code format)
   - `Eval.from_agent_file(path, provider, ...)` ‚Äî agent file becomes the system prompt; test it synthetically
   - `load_custom_agent(path)` + `CopilotEval(custom_agents=[...])` ‚Äî test real subagent dispatch through Copilot
   - Both approaches are first-class: same prominence as Skill Testing

7. **Clarification Detection**: Catch evals that ask questions instead of acting
   - LLM-as-judge detects "Would you like me to...?" style responses
   - Configure with `ClarificationDetection(enabled=True)` on Eval
   - Assert with `result.asked_for_clarification` / `result.clarification_count`
   - Levels: INFO (log only), WARNING (default), ERROR (fail test)
   - Uses separate judge LLM call (defaults to agent's own model)

### AI Analysis (KEY DIFFERENTIATOR)

Reports are **insights-first**, not metrics-first. AI analysis is **mandatory** when generating reports.

Reports include:
- **üéØ Recommendation**: Deploy recommendation with cost/performance analysis
- **‚ùå Failure Analysis**: Root cause + suggested fix for each failure
- **üîß MCP Tool Feedback**: Improve tool descriptions, with copy button
- **üìù Prompt Feedback**: System prompt improvements
- **üìö Skill Feedback**: Skill restructuring suggestions
- **‚ö° Optimizations**: Reduce turns/tokens

```bash
# Run tests with AI analysis (mandatory --aitest-summary-model)
pytest tests/ --aitest-html=report.html --aitest-summary-model=azure/gpt-5.2-chat

# Regenerate report with new AI insights from existing JSON (no re-run)
pytest-skill-engineering-report results.json --html report.html --summary --summary-model azure/gpt-5-mini
```

### Key Types

```python
from pytest_skill_engineering import Eval, Provider, MCPServer, Skill, load_system_prompts, load_custom_agent

# Define an eval (auth via AZURE_API_BASE env var)
agent = Eval(
    provider=Provider(model="azure/gpt-5-mini"),
    mcp_servers=[my_server],
    system_prompt="You are helpful...",
    skill=Skill.from_path("skills/financial-advisor"),  # Optional domain knowledge
    max_turns=10,
)

# Load a custom agent file as the agent under test (synthetic testing)
agent = Eval.from_agent_file(
    "skills/my-skill/agent.md",
    provider=Provider(model="azure/gpt-5-mini"),
    mcp_servers=[my_server],
)

# Load a custom agent as a subagent for CopilotEval (real Copilot dispatch)
from pytest_skill_engineering.copilot import CopilotEval
copilot_agent = CopilotEval(
    custom_agents=[load_custom_agent("skills/my-skill/agent.md")],
)

# Load system prompts from .md files - returns dict[str, str]
prompts = load_system_prompts(Path("prompts/"))
# {"concise": "Be brief...", "detailed": "Explain..."}

# Run test
result = await eval_run(agent, "Do something with tools")
assert result.success
assert result.tool_was_called("my_tool")
assert not result.asked_for_clarification  # Eval should act, not ask
```

### Multi-Turn Sessions

```python
@pytest.mark.session("banking-flow")
class TestBankingWorkflow:
    async def test_check_balance(self, eval_run, bank_agent):
        result = await eval_run(bank_agent, "What's my balance?")
        assert result.success

    async def test_transfer(self, eval_run, bank_agent):
        # Shares context with previous test
        result = await eval_run(bank_agent, "Transfer $100 to savings")
        assert result.tool_was_called("transfer")
```

### System Prompts

System prompts can be plain `.md` files or YAML files with metadata.

```python
# Load .md files as plain strings - returns dict[str, str]
prompts = load_system_prompts(Path("prompts/"))
# {"concise": "Be brief...", "detailed": "Explain..."}

# Or load .yaml files as Prompt objects - returns list[Prompt]
from pytest_skill_engineering import load_prompts
prompt_list = load_prompts(Path("prompts/"))

# Use with pytest parametrize
@pytest.mark.parametrize("prompt_name,system_prompt", prompts.items())
async def test_with_prompt(eval_run, banking_server, prompt_name, system_prompt):
    agent = Eval(
        provider=Provider(model="azure/gpt-5-mini"),
        mcp_servers=[banking_server],
        system_prompt=system_prompt,
    )
    result = await eval_run(agent, "What's my balance?")
    assert result.success
```

## CRITICAL: Testing Philosophy

**Unit tests with mocks are WORTHLESS for this project. Do NOT run them.**

This is a testing framework that validates AI interfaces (MCP tool descriptions, system prompts, skills, custom agents). A mock that pretends to be an LLM proves nothing ‚Äî you're testing the mock, not the AI interface.

**The only valid test is a real LLM call against real tools.**

### When you make a code change ‚Äî run integration tests immediately:

```bash
# Run ALL pydantic integration tests ‚Äî do this after every change
uv run python -m pytest tests/integration/pydantic/ -v

# Run a specific test file
uv run python -m pytest tests/integration/pydantic/test_01_basic.py -v

# Re-run only failed tests after a full run
uv run python -m pytest --lf tests/integration/pydantic/ -v
```

**Always use `uv run python -m pytest`** ‚Äî bare `pytest` won't find the installed package.

**Fast test execution (< 1 second) is a RED FLAG.** Real LLM calls take time.

### What NOT to do:
- ‚ùå **NEVER run `tests/unit/` to validate a feature** ‚Äî unit tests do not validate AI behavior
- ‚ùå **NEVER declare a feature "done" based on unit tests passing**
- ‚ùå **NEVER say "all tests pass" when you only ran unit tests**
- ‚ùå Do NOT write unit tests with mocked LLM responses
- ‚ùå Do NOT use `unittest.mock.patch` on PydanticAI or agent execution

### What TO do:
- ‚úÖ Run `tests/integration/pydantic/` after EVERY code change ‚Äî one file at a time, sequentially
- ‚úÖ Start with `test_01_basic.py`, fix failures, then `test_02_models.py`, etc.
- ‚úÖ Write integration tests that call real Azure OpenAI models
- ‚úÖ Use the cheapest model (`gpt-5-mini`) via Azure
- ‚úÖ Test with Banking or Todo MCP server (built-in test harnesses)
- ‚úÖ Accept that integration tests take 5‚Äì30+ seconds per test
- ‚úÖ Run integration tests BEFORE declaring a feature complete

## CRITICAL: No Such Thing as a Pre-Existing Failure

**Every test failure is YOUR responsibility to fix. There are no "pre-existing" or "unrelated" failures.**

- NEVER skip a failing test because it was failing before your change
- NEVER label failures as "pre-existing" and move on
- NEVER say "this failure is unrelated to my change"
- If a test fails, fix it ‚Äî full stop
- If a test is wrong (not the code), fix the test
- The baseline must be **zero failures** before and after your change

## CRITICAL: Efficient Test Execution

**Integration tests are EXPENSIVE. Never re-run passing tests unnecessarily.**

### pytest caching commands:
```bash
# Run ONLY tests that failed last time (MOST COMMON)
uv run python -m pytest --lf tests/integration/pydantic/

# Run failed tests first, then the rest
uv run python -m pytest --ff tests/integration/pydantic/

# Check what's in the cache (see last failures)
uv run python -m pytest --cache-show

# Clear the cache (fresh start)
uv run python -m pytest --cache-clear

# Run specific failing test(s) only
uv run python -m pytest tests/integration/pydantic/test_01_basic.py::TestBankingBasic::test_balance_check_and_transfer -v
```

### Rules for the AI assistant:
1. **After making changes, ALWAYS run pydantic integration tests** ‚Äî `tests/integration/pydantic/`, never `tests/unit/`
2. **Run test files ONE AT A TIME, sequentially** ‚Äî start with `test_01_basic.py`, fix all failures, then move to `test_02_models.py`, etc.
3. **After fixing a test, run ONLY that specific test** to confirm
4. **Use `--lf` to re-run only failed tests** after a full run
5. **Fix ALL failures** ‚Äî no exceptions, no "pre-existing" excuses
6. **Quote the specific test paths when running individual tests**
7. **Do NOT say "tests pass" without running `tests/integration/pydantic/`**
8. **NEVER run `tests/unit/` as validation** ‚Äî unit tests prove nothing in this project

## Azure Configuration

**Endpoint**: `https://stbrnner1.cognitiveservices.azure.com/`
**Resource Group**: `rg_foundry`
**Account**: `stbrnner1`

**Authentication**: Entra ID (automatic via `az login`). No API keys needed!
The engine uses `core.auth.get_azure_ad_token_provider()` internally (shared module).

Available models (checked 2026-02-01):
- `gpt-5-mini` - CHEAPEST, use for most tests
- `gpt-5.2-chat` - More capable
- `gpt-4.1` - Most capable

Check for updates:
```bash
az cognitiveservices account deployment list \
  --name stbrnner1 \
  --resource-group rg_foundry \
  -o table
```

## Copilot Model Provider

Use the `copilot/` prefix to route LLM calls through the Copilot SDK instead of Azure/OpenAI.
Requires `pytest-skill-engineering[copilot]` and Copilot auth (`gh auth login` or `GITHUB_TOKEN`).

```bash
# Use Copilot for AI insights
pytest tests/ --aitest-summary-model=copilot/gpt-5-mini

# Use Copilot for llm_assert / llm_score
pytest tests/ --llm-model=copilot/gpt-5-mini
```

Handled in `build_model_from_string()` in `pydantic_adapter.py` ‚Äî creates a `CopilotModel` from `copilot/model.py`.
Available models are dynamic (whatever Copilot exposes).

## Project Structure

```
src/pytest_skill_engineering/
‚îú‚îÄ‚îÄ core/                  # Core types
‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # Eval, Provider, MCPServer, CLIServer, Wait
‚îÇ   ‚îú‚îÄ‚îÄ auth.py            # Shared Azure AD auth (get_azure_ad_token_provider)
‚îÇ   ‚îú‚îÄ‚îÄ prompt.py          # load_system_prompts() for .md files (returns dict[str, str])
‚îÇ   ‚îú‚îÄ‚îÄ result.py          # EvalResult, Turn, ToolCall, ToolInfo, SkillInfo
‚îÇ   ‚îú‚îÄ‚îÄ skill.py           # Skill, load from markdown
‚îÇ   ‚îî‚îÄ‚îÄ errors.py          # AITestError, ServerStartError, etc.
‚îú‚îÄ‚îÄ execution/             # Runtime
‚îÇ   ‚îú‚îÄ‚îÄ engine.py          # EvalEngine (PydanticAI-powered agent execution)
‚îÇ   ‚îú‚îÄ‚îÄ pydantic_adapter.py # Adapter: our config types ‚Üî PydanticAI types (handles azure/, copilot/ prefixes)
‚îÇ   ‚îú‚îÄ‚îÄ cli_toolset.py     # Custom PydanticAI Toolset for CLI servers
‚îÇ   ‚îú‚îÄ‚îÄ clarification.py   # Clarification detection via pydantic-evals LLMJudge
‚îÇ   ‚îú‚îÄ‚îÄ servers.py         # Server process management
‚îÇ   ‚îî‚îÄ‚îÄ skill_tools.py     # Skill injection into agent
‚îú‚îÄ‚îÄ fixtures/              # Pytest fixtures
‚îÇ   ‚îú‚îÄ‚îÄ run.py             # eval_run fixture
‚îÇ   ‚îî‚îÄ‚îÄ factories.py       # skill_factory (Skills only - agents created inline)
‚îú‚îÄ‚îÄ reporting/             # AI analysis & reports
‚îÇ   ‚îú‚îÄ‚îÄ collector.py       # TestReport, SuiteReport dataclasses + build_suite_report()
‚îÇ   ‚îú‚îÄ‚îÄ generator.py       # generate_html(), generate_json(), generate_mermaid_sequence()
‚îÇ   ‚îú‚îÄ‚îÄ insights.py        # AI analysis engine (PydanticAI-powered) ‚Üí InsightsResult
‚îÇ   ‚îî‚îÄ‚îÄ components/        # htpy report components
‚îÇ       ‚îú‚îÄ‚îÄ types.py       # TypedDicts for component data shapes
‚îÇ       ‚îú‚îÄ‚îÄ report.py      # Full report layout
‚îÇ       ‚îú‚îÄ‚îÄ agent_leaderboard.py  # Eval ranking table
‚îÇ       ‚îú‚îÄ‚îÄ agent_selector.py     # Eval comparison toggles
‚îÇ       ‚îú‚îÄ‚îÄ test_grid.py          # Test results grid
‚îÇ       ‚îú‚îÄ‚îÄ test_comparison.py    # Side-by-side agent comparison
‚îÇ       ‚îî‚îÄ‚îÄ overlay.py            # Fullscreen expanded view
‚îú‚îÄ‚îÄ templates/             # Static assets for HTML reports
‚îÇ   ‚îî‚îÄ‚îÄ partials/          # Static assets
‚îÇ       ‚îú‚îÄ‚îÄ report.css     # Hand-written CSS (edit directly)
‚îÇ       ‚îî‚îÄ‚îÄ scripts.js     # JS (Mermaid, copy buttons, filtering)
‚îî‚îÄ‚îÄ testing/               # Test harnesses
    ‚îú‚îÄ‚îÄ todo.py            # TodoStore for CRUD tests
    ‚îú‚îÄ‚îÄ todo_mcp.py        # Todo MCP server
    ‚îú‚îÄ‚îÄ banking.py         # BankingService for financial tests
    ‚îî‚îÄ‚îÄ banking_mcp.py     # Banking MCP server

tests/
‚îú‚îÄ‚îÄ integration/           # REAL LLM tests (the only tests that matter)
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py        # Constants + server fixtures (shared by both harnesses)
‚îÇ   ‚îú‚îÄ‚îÄ pydantic/          # Eval harness tests (BYOM via PydanticAI, USD costs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py    # Minimal, inherits from parent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_01_basic.py       # Single eval, basic MCP tool calls
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_02_models.py      # Model comparison (parametrize)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_03_prompts.py     # System prompt comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_04_matrix.py      # Model √ó Prompt 2√ó2 grid
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_05_skills.py      # Skill loading + skill-enhanced behavior
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_06_sessions.py    # Multi-turn sessions (@pytest.mark.session)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_07_clarification.py # ClarificationDetection feature
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_08_scoring.py     # LLM scoring (llm_score + ScoringDimension)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_09_cli.py         # CLIServer wrapping shell commands
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_10_ab_servers.py  # A/B server comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_11_iterations.py  # --aitest-iterations=N reliability
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_12_custom_agents.py # Eval.from_agent_file + load_custom_agent
‚îÇ   ‚îú‚îÄ‚îÄ agents/            # .agent.md test fixtures (banking-advisor, todo-manager, minimal)
‚îÇ   ‚îú‚îÄ‚îÄ copilot/           # CopilotEval harness tests (Copilot SDK, premium requests)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py    # MODELS, DEFAULT_MODEL, integration_judge_model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_01_basic.py       # File create + refactor (parametrize models)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_02_models.py      # Model comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_03_instructions.py # Instruction differentiation + excluded_tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_05_skills.py      # Skill A/B comparison
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_12_custom_agents.py # Custom agents + forced subagent dispatch
‚îÇ   ‚îú‚îÄ‚îÄ prompts/           # Plain .md system prompt files
‚îÇ   ‚îî‚îÄ‚îÄ skills/            # Test skills
‚îî‚îÄ‚îÄ unit/                  # Pure logic only (no mocking LLMs)
```

## Test Configuration (conftest.py)

Integration tests use centralized constants from `tests/integration/conftest.py`:

```python
# Models
DEFAULT_MODEL = "gpt-5-mini"           # Cheapest, use for most tests
BENCHMARK_MODELS = ["gpt-5-mini", "gpt-4.1-mini"]  # For model comparison

# Rate limits (Azure deployments)
DEFAULT_RPM = 10
DEFAULT_TPM = 10000

# Turn limits
DEFAULT_MAX_TURNS = 5

# Server fixtures: todo_server, banking_server
# Evals are created INLINE in each test using these constants
```

**Pattern for writing pydantic tests** (in `tests/integration/pydantic/`):
```python
from pytest_skill_engineering import Eval, Provider
from ..conftest import DEFAULT_MODEL, DEFAULT_RPM, DEFAULT_TPM, DEFAULT_MAX_TURNS, BANKING_PROMPT

async def test_balance(eval_run, banking_server):
    agent = Eval.from_instructions(
        "banking-test",
        BANKING_PROMPT,
        provider=Provider(model=f"azure/{DEFAULT_MODEL}", rpm=DEFAULT_RPM, tpm=DEFAULT_TPM),
        mcp_servers=[banking_server],
        max_turns=DEFAULT_MAX_TURNS,
    )
    result = await eval_run(agent, "What's my checking balance?")
    assert result.success
```

**Pattern for writing copilot tests** (in `tests/integration/copilot/`):
```python
from pytest_skill_engineering.copilot.eval import CopilotEval
from .conftest import MODELS

@pytest.mark.parametrize("model", MODELS)
async def test_create_file(copilot_eval, tmp_path, model):
    agent = CopilotEval(
        name=f"coder-{model}",
        model=model,
        instructions="You are a Python developer.",
        working_directory=str(tmp_path),
    )
    result = await copilot_eval(agent, "Create hello.py with print('hello')")
    assert result.success
```

**CRITICAL: Never mix harnesses.** Pydantic tests (using `eval_run`) and Copilot tests
(using `copilot_eval`) must be in separate directories. The plugin enforces this at
collection time ‚Äî if both fixtures appear in a single session, `pytest.UsageError` is raised.

## Semantic Assertions with llm_assert

Use the built-in `llm_assert` fixture for AI-powered semantic assertions (powered by `pydantic-evals` LLMJudge):

```python
async def test_response_quality(eval_run, banking_server, llm_assert):
    agent = Eval(...)
    result = await eval_run(agent, "Show me my account balances and recent transactions")
    
    # Semantic assertion - AI evaluates if condition is met
    assert llm_assert(result.final_response, "includes account balances and transaction details")
```

## CRITICAL: Report Development

### Report Generation Architecture

The report pipeline flows:  
**Test Execution ‚Üí Plugin (list[TestReport]) ‚Üí build_suite_report() ‚Üí AI Insights ‚Üí generate_html()/generate_json() ‚Üí HTML/JSON**

```
src/pytest_skill_engineering/reporting/
‚îú‚îÄ‚îÄ collector.py       # TestReport, SuiteReport dataclasses + build_suite_report()
‚îú‚îÄ‚îÄ insights.py        # AI analysis engine (mandatory) ‚Üí InsightsResult
‚îú‚îÄ‚îÄ generator.py       # generate_html(), generate_json() module-level functions
‚îî‚îÄ‚îÄ components/        # htpy report components + types.py
```

### Data Contracts (IMPORTANT)

Every htpy component has a **typed data contract** in `components/types.py`. This is the explicit interface between Python and components:

| Contract | Used By | Purpose |
|----------|---------|---------|
| `AgentData` | `agent_leaderboard.py`, `agent_selector.py` | Eval metrics: pass_rate, cost, tokens, is_winner |
| `TestResultData` | `test_comparison.py`, `test_grid.py` | Per-agent test result: tool_calls, mermaid, outcome |
| `TestData` | `test_grid.py` | Test with `results_by_agent` dict |
| `TestGroupData` | `test_grid.py` | Session or standalone group containing tests |
| `ReportContext` | `report.py` | Full report context with all data + helper functions |

**Pattern**: When modifying components, always check `components/types.py` for the expected shape. When adding component fields, add them to the contract first.

### Template Structure

```
src/pytest_skill_engineering/templates/
‚îî‚îÄ‚îÄ partials/
    ‚îú‚îÄ‚îÄ report.css           # Hand-written CSS (edit directly)
    ‚îî‚îÄ‚îÄ scripts.js           # JS: Mermaid, copy buttons, expand/collapse, eval filtering
```

**Note:** HTML rendering is handled by htpy components in `reporting/components/`, not by template files.

### CSS Development

The project uses hand-written CSS in `partials/report.css`. No build step needed ‚Äî edit and regenerate reports.

The CSS provides:
- Design tokens as CSS custom properties (colors, shadows, radii, fonts)
- Utility classes matching the patterns used by htpy components
- Semantic component classes (`.card`, `.leaderboard-table`, `.agent-chip`, etc.)
- Markdown content styling

To modify styles:
1. Edit `src/pytest_skill_engineering/templates/partials/report.css`
2. Regenerate report: `uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html`
3. Open in browser and verify

### Report Sources

1. **Integration tests demonstrate capabilities** - Each test file shows a feature:
   - `test_01_basic.py` ‚Üí Basic tool usage
   - `test_02_models.py` ‚Üí Model comparison leaderboard
   - `test_03_prompts.py` ‚Üí System prompt comparison
   - `test_04_matrix.py` ‚Üí Model √ó System Prompt grid
   - `test_05_skills.py` ‚Üí Skills integration
   - `test_06_sessions.py` ‚Üí Multi-turn sessions
   - `test_07_clarification.py` ‚Üí Clarification detection
   - `test_08_scoring.py` ‚Üí LLM scoring / rubrics
   - `test_09_cli.py` ‚Üí CLI server testing
   - `test_10_ab_servers.py` ‚Üí A/B server comparison
   - `test_11_iterations.py` ‚Üí Iteration reliability
   - `test_12_custom_agents.py` ‚Üí Custom agent files (Eval.from_agent_file)

2. **Showcase tests for hero report** - Located in `tests/showcase/`:
   - `test_hero.py` ‚Üí Curated tests for README showcase
   - Demonstrates ALL capabilities in a single cohesive report
   - Output: `docs/demo/hero-report.html` (committed to repo)

3. **Report config is in pyproject.toml**:
   ```toml
   addopts = """
   --aitest-summary-model=azure/gpt-5.2-chat
   --aitest-html=aitest-reports/report.html
   """
   ```

### Generate Reports

```bash
# Pydantic integration tests (development)
uv run python -m pytest tests/integration/pydantic/ -v

# Hero report for README showcase
uv run python -m pytest tests/showcase/ -v --aitest-html=docs/demo/hero-report.html
```

### Manual Testing Workflow (Pre-PR)

**Integration tests use real LLM calls and are expensive. Run the relevant integration tests after every change.**

```bash
# 1. Run the pydantic integration test(s) that cover your change
uv run python -m pytest tests/integration/pydantic/test_01_basic.py -v

# 2. Run all failed tests (if a broader run previously failed)
uv run python -m pytest --lf tests/integration/pydantic/ -v

# 3. Generate hero report (before major releases)
uv run python -m pytest tests/showcase/ -v --aitest-html=docs/demo/hero-report.html
```

## CRITICAL: Template/Report Changes - NEVER Re-run Tests

**For template, CSS, JS, or report generation changes:**

```bash
# CORRECT - Regenerate HTML from existing JSON (instant, no LLM cost)
uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html

# WRONG - Never re-run tests just to see template changes!
# uv run pytest tests/showcase/ ...  ‚Üê DON'T DO THIS
```

**When to re-run tests:**
- Test code itself changed
- Eval/tool/engine functionality changed
- Need fresh data with different models/prompts

**When to regenerate from JSON:**
- Template changes (HTML, CSS, JS)
- Report generator code changes
- Layout/styling experiments
- Data contract changes (if backward compatible)

**Workflow for template development:**
1. Edit CSS in `src/pytest_skill_engineering/templates/partials/report.css` or htpy components
2. Regenerate report from existing JSON:
   ```bash
   uv run pytest-skill-engineering-report aitest-reports/results.json --html aitest-reports/test.html
   ```
3. Open the HTML file in browser
4. Repeat steps 1-3 until satisfied

This workflow is **instant and free** - no LLM calls, no API costs.

### Key Files for Report Development

| File | When to Modify |
|------|----------------|
| `reporting/components/types.py` | Adding new data fields for components |
| `reporting/generator.py` | Changing how context is built for components |
| `reporting/components/*.py` | Individual UI components (htpy) |
| `templates/partials/scripts.js` | Interactivity, Mermaid diagrams |
| `templates/partials/report.css` | Styles, colors, layout |
| `cli.py` | CLI commands for report regeneration |

### Report Design Principles

- **Material Design** - Match mkdocs-material indigo theme
- **Roboto fonts** - Via Google Fonts
- **Test details expanded by default** - Users want to see results immediately
- **Human-readable names everywhere** - All user-facing reports (HTML, Markdown) must use human-readable names, not internal IDs. Use docstrings or humanize function names for tests. Use `eval_name` (not `agent_id`) for evals. Use `system_prompt_name` (not raw keys). If a human-readable name isn't available, humanize the ID (e.g., `test_check_balance` ‚Üí `Test check balance`).
- **Assertions visible** - Show tool_was_called, semantic assertions
- **Mermaid diagrams readable** - Use neutral theme with good contrast
- **AI insights prominent** - Verdict section at top with clear recommendation
- **Contract-first development** - Define TypedDict before touching templates

### Markdown Style Guidelines

- **No horizontal rules (`---`)** - Headings provide sufficient visual separation
- Use `##` headings for major sections, `###` for subsections
- Horizontal rules inside code blocks are fine (e.g., YAML frontmatter examples)


